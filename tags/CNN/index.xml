<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CNN on jdhao's digital space</title><link>https://jdhao.github.io/tags/CNN/</link><description>Recent content in CNN on jdhao's digital space</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>jdhao</copyright><lastBuildDate>Fri, 29 Sep 2017 09:40:00 +0800</lastBuildDate><atom:link href="https://jdhao.github.io/tags/CNN/index.xml" rel="self" type="application/rss+xml"/><item><title>1x1 Convolutions Demystified</title><link>https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/</link><pubDate>Fri, 29 Sep 2017 09:40:00 +0800</pubDate><guid>https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/</guid><description>In the early development of convolutional neural networks (CNNs),
convolutions with kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are often used.
In the more recent literature, however, $1\times 1$ convolutions are becoming prevalent.
In this post, I will try to explain what $1\times 1$ convolutions are and discuss why they are used in CNNs.</description></item><item><title>神经网络中误差反向传播(back propagation)算法的工作原理</title><link>https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/</link><pubDate>Tue, 19 Jan 2016 00:00:00 +0800</pubDate><guid>https://jdhao.github.io/2016/01/19/back-propagation-in-mlp-explained/</guid><description>神经网络，从大学时期就听说过，后面上课的时候老师也讲过，但是感觉从来没有真正掌握，总是似是而非，比较模糊，好像懂，其实并不懂。
为了搞懂神经网络的运行原理，有必要搞清楚神经网络最核心的算法，也就是[误差反向传播算法](https://en.wikipedia.org/wiki/Backpropagation)的工作原理，本文以最简单的全连接神经网络为例，介绍误差反向传播算法的原理。</description></item></channel></rss>