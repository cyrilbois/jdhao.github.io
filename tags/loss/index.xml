<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>loss on jdhao&#39;s blog</title>
    <link>https://jdhao.github.io/tags/loss/</link>
    <description>Recent content in loss on jdhao&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>jdhao</copyright>
    <lastBuildDate>Mon, 13 Mar 2017 10:14:55 +0800</lastBuildDate>
    
	<atom:link href="https://jdhao.github.io/tags/loss/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Some Loss Functions and Their Intuitive Explanations</title>
      <link>https://jdhao.github.io/2017/03/13/some_loss_and_explanations/</link>
      <pubDate>Mon, 13 Mar 2017 10:14:55 +0800</pubDate>
      
      <guid>https://jdhao.github.io/2017/03/13/some_loss_and_explanations/</guid>
      <description>&lt;p&gt;Loss functions are frequently used in supervised machine learning to minimize the differences between the predicted output of the model and the ground truth labels. In other words, it is used to measure how good our model can predict the true class of a sample from the dataset. Here I would like to list some frequently-used loss functions and give my intuitive explanation.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>