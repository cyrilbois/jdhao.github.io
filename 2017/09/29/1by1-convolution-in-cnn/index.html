<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>1x1 Convolutions Demystified - jdhao's digital space</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="jdhao"><meta name=description content="In the early development of convolutional neural networks (CNNs),
convolutions with kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are often used.
In the more recent literature, however, $1\times 1$ convolutions are becoming prevalent.
In this post, I will try to explain what $1\times 1$ convolutions are and discuss why they are used in CNNs."><meta name=keywords content="Hugo,theme,even"><meta name=google-site-verification content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw"><meta name=generator content="Hugo 0.105.0 with theme even"><link rel=canonical href=https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script>
<script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-6058871559165202",enable_page_level_ads:!0})</script><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="1x1 Convolutions Demystified"><meta property="og:description" content="In the early development of convolutional neural networks (CNNs),
convolutions with kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are often used.
In the more recent literature, however, $1\times 1$ convolutions are becoming prevalent.
In this post, I will try to explain what $1\times 1$ convolutions are and discuss why they are used in CNNs."><meta property="og:type" content="article"><meta property="og:url" content="https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-09-29T09:40:00+08:00"><meta property="article:modified_time" content="2022-03-21T23:33:00+08:00"><meta itemprop=name content="1x1 Convolutions Demystified"><meta itemprop=description content="In the early development of convolutional neural networks (CNNs),
convolutions with kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are often used.
In the more recent literature, however, $1\times 1$ convolutions are becoming prevalent.
In this post, I will try to explain what $1\times 1$ convolutions are and discuss why they are used in CNNs."><meta itemprop=datePublished content="2017-09-29T09:40:00+08:00"><meta itemprop=dateModified content="2022-03-21T23:33:00+08:00"><meta itemprop=wordCount content="557"><meta itemprop=keywords content="CNN,"><meta name=twitter:card content="summary"><meta name=twitter:title content="1x1 Convolutions Demystified"><meta name=twitter:description content="In the early development of convolutional neural networks (CNNs),
convolutions with kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are often used.
In the more recent literature, however, $1\times 1$ convolutions are becoming prevalent.
In this post, I will try to explain what $1\times 1$ convolutions are and discuss why they are used in CNNs."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>jdhao's digital space</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>jdhao's digital space</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>1x1 Convolutions Demystified</h1><div class=post-meta><span class=post-time>2017-09-29</span><div class=post-category><a href=/categories/machine-learning/>machine-learning</a></div><span class=more-meta>557 words</span>
<span class=more-meta>3 mins read</span>
<span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class=post-toc-content></div></div><div class=post-content>In the early development of convolutional neural networks (CNNs),
convolutions with kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are often used.
In the more recent literature, however, $1\times 1$ convolutions are becoming prevalent.
In this post, I will try to explain what $1\times 1$ convolutions are and discuss why they are used in CNNs.
The use of $1\times 1$ convolutions is mainly for dimension reduction in channels
to reduce the computational cost of the following $3\times 3$ or $5\times 5$ convolutions.
Another purpose is to increase the non-linearity as $1\times 1$ convolutions are immediately followed by ReLU.
The two purposes of $1\times 1$ convolutions are stated clearly in this paper:
> That is, $1\times 1$ convolutions are used to compute reductions before the
> expensive $3\times 3$ and $5\times 5$ convolutions. Besides being used as
> reductions, they also include the use of rectified linear activation making
> them dual-purpose.
In the more recent [ResNet](https://arxiv.org/abs/1512.03385) paper, the authors proposed to use "bottleneck" structure (shown below),
which also involves the use of $1\times 1$ convolutions.
The purpose of it is also for reducing or increasing dimensions.<p align=center><img src=https://blog-resource-1257868508.file.myqcloud.com/resnet_bottleneck.png width=300></p># Relationship between fully-connected layers and $1\times 1$ convolutions
In image classification, researchers often use fixed size input image and fully
connected layers as classifiers to classify the image. But according to the
father of convolutional neural networks -- Yann lecun, there is no such thing
as fully-connected layers and you do not need to use fixed size input image in
testing time:
> In Convolutional Nets, there is no such thing as "fully-connected layers".
> There are only convolution layers with 1x1 convolution kernels and a full
> connection table.
> It's a too-rarely-understood fact that ConvNets don't need to have a fixed-size
> input. You can train them on inputs that happen to produce a single output
> vector (with no spatial extent), and then apply them to larger images. Instead
> of a single output vector, you then get a spatial map of output vectors. Each
> vector sees input windows at different locations on the input. In that
> scenario, the "fully connected layers" really act as 1x1 convolutions.
That is, we can interpret fully-connected layers as $1\times 1$ convolutions.
For example, in VGG-16 network, the input image size is $224\times 224$, the
output is a classification score for 1000 classes, whose size is $1000\times
1\times 1$. If the input images have bigger size than $224\times 224$, we can
interpret the fully-connected layers as convolutions in a sliding windows
fashion so that the output of the network is a classification map of size
$1000\times W\times H$, where $W>1$ and $H>1$. Each point in each
classification map represent the probability of the corresponding receptive
field in the original image belonging to that specific class.
# Conclusions
In summary, $1\times 1$ convolutions are used for 2 purposes:
ones is to reduce dimensions in order to reduce computational cost,
the other is to increase non-linearity of the network (or the capacity of the network) in a cheap way.
# References
+ [A discussion about the meaning of $1\times 1$ convolutions](https://groups.google.com/forum/#!topic/caffe-users/f1R-JrUQSMg)
+ [Why $1\times 1$ convolutions are used](https://stackoverflow.com/q/39366271/6064933)
+ [Another post about the meaning of $1\times 1$ convolutions](https://stats.stackexchange.com/q/194142/140049)
+ [A discussion about the significance of *network in network*](https://www.reddit.com/r/MachineLearning/comments/5n01i4/d_network_in_network_nin_is_it_still_useful/)
+ [How to interpret the fully-connected layers as convolutions](https://qr.ae/pG08uW).</div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>jdhao</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-03-21</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/CNN/>CNN</a></div><nav class=post-nav><a class=prev href=/2017/09/30/sorting-algorithms-stability/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">When Does the Stability of Sorting Algorithms Matter?</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/2017/09/24/some-good-android-apps/><span class="next-text nav-default">善用佳软---我常用的 Android 应用</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=jdhao/jdhao.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:jdhao@hotmail.com class="iconfont icon-email" title=email></a>
<a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title=stack-overflow></a>
<a href=https://github.com/jdhao class="iconfont icon-github" title=github></a>
<a href=https://jdhao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span></span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv>site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span></span></div><span class=copyright-year>&copy;
2017 -
2022<span class=heart><i class="iconfont icon-heart"></i></span><span>jdhao</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-113395108-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script id=baidu_push>(function(){if(window.location.hostname==="localhost")return;var t,n,e=document.createElement("script");e.async=!0,n=window.location.protocol.split(":")[0],n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>