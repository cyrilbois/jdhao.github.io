<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>1x1 Convolutions Demystified - jdhao&#39;s blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="jdhao" /><meta name="description" content="In the early development of convolutional neural networks (CNNs), convolutions with kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are often used. In the more recent literature, however, $1\times 1$ convolutions are becoming prevalent. In this post, I will try to explain what $1\times 1$ convolutions are and discuss why they are used in CNNs.
" /><meta name="keywords" content="Hugo, theme, even" />



<meta name="google-site-verification" content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw" />


<meta name="generator" content="Hugo 0.74.3 with theme even" />


<link rel="canonical" href="https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6058871559165202",
    enable_page_level_ads: true
  });
</script>




<link href="/sass/main.min.651e6917abb0239242daa570c2bec9867267bbcd83646da5a850afe573347b44.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="1x1 Convolutions Demystified" />
<meta property="og:description" content="In the early development of convolutional neural networks (CNNs), convolutions with
kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are
often used. In the more recent literature, however, $1\times 1$ convolutions
are becoming prevalent. In this post, I will try to explain what $1\times 1$
convolutions are and discuss why they are used in CNNs." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/" />
<meta property="article:published_time" content="2017-09-29T09:40:00+08:00" />
<meta property="article:modified_time" content="2020-03-11T23:27:56+08:00" />
<meta itemprop="name" content="1x1 Convolutions Demystified">
<meta itemprop="description" content="In the early development of convolutional neural networks (CNNs), convolutions with
kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are
often used. In the more recent literature, however, $1\times 1$ convolutions
are becoming prevalent. In this post, I will try to explain what $1\times 1$
convolutions are and discuss why they are used in CNNs.">
<meta itemprop="datePublished" content="2017-09-29T09:40:00+08:00" />
<meta itemprop="dateModified" content="2020-03-11T23:27:56+08:00" />
<meta itemprop="wordCount" content="764">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="1x1 Convolutions Demystified"/>
<meta name="twitter:description" content="In the early development of convolutional neural networks (CNNs), convolutions with
kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are
often used. In the more recent literature, however, $1\times 1$ convolutions
are becoming prevalent. In this post, I will try to explain what $1\times 1$
convolutions are and discuss why they are used in CNNs."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">jdhao&#39;s blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">jdhao&#39;s blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">1x1 Convolutions Demystified</h1>

      <div class="post-meta">
        <span class="post-time"> 2017-09-29 </span>
        <div class="post-category">
            <a href="/categories/academic/"> academic </a>
            </div>
          <span class="more-meta"> 764 words </span>
          <span class="more-meta"> 4 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#the-meaning-of-1times-1-convolutions">The meaning of $1\times 1$ convolutions</a></li>
    <li><a href="#relationship-between-fully-connected-layers-and-1times-1-convolutions">Relationship between fully-connected layers and $1\times 1$ convolutions</a></li>
    <li><a href="#conclusions">Conclusions</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>In the early development of convolutional neural networks (CNNs), convolutions with
kernel size $3\times 3$, $5\times 5$, $7\times 7$ or even $11\times 11$ are
often used. In the more recent literature, however, $1\times 1$ convolutions
are becoming prevalent. In this post, I will try to explain what $1\times 1$
convolutions are and discuss why they are used in CNNs.</p>
<h1 id="the-meaning-of-1times-1-convolutions">The meaning of $1\times 1$ convolutions</h1>
<p>In essence, $1\times 1$ convolutions are just convolutions with kernel size
equal to $1$, nothing new. Suppose the feature map size of one layer is
$64\times 12\times 12$, and it is followed by $1\times 1$ convolutions, and the
output channel is $C$. Then in this convolution layer, we will have $C$
filters, each of which has size $64\times 1\times 1$. The convolutional weight
shape in this layer is $C\times 64\times 1\times 1$, and the shape of bias
terms is $C\times 1\times 1$. The output feature map size after this
convolutional layer is $C\times 12\times 12$. So the spatial size of feature
maps is unchanged but the number of feature maps maybe be changed depending on
whether $C$ is bigger or smaller than input channel num.</p>
<p>The paper <a href="https://arxiv.org/abs/1312.4400"><em>Network in network</em></a> first
proposed to use $1\times 1$ convolutions. In this paper, multiple $1\times 1$
convolutional layers is stacked together. The author call it a <em>mlplayer</em>.
Since the kernel size is $1\times 1$, $1\times 1$ convolution can not learn a
spatial relationship within channels and is used solely to learn cross-channel
relations of the feature maps. You can also think of it as a fully-connected
operation where for each feature map the multiplying coefficients are the same.</p>
<p>Later in the Google <a href="https://arxiv.org/abs/1409.4842">Inception</a> paper,
$1\times 1$ convolutions are also used in the inception module shown below.</p>
<img src="https://blog-resource-1257868508.file.myqcloud.com/google_inception-V1.png">
<p>The use of $1\times 1$ convolutions is mainly for dimension reduction in
channels to reduce the computational cost of the following $3\times 3$ or
$5\times 5$ convolutions. Another purpose is for increasing the non-linearity
as $1\times 1$ convolutions are immediately followed by ReLU. The two purposes
of $1\times 1$ convolutions are stated clearly in this paper:</p>
<blockquote>
<p>That is, $1\times 1$ convolutions are used to compute reductions before the
expensive $3\times 3$ and $5\times 5$ convolutions. Besides being used as
reductions, they also include the use of rectified linear activation making
them dual-purpose.</p>
</blockquote>
<p>In the more recent <a href="https://arxiv.org/abs/1512.03385">ResNet</a> paper, the
authors proposed to use &ldquo;bottleneck&rdquo; structure (shown below) which also
involves the use of $1\times 1$ convolutionss. The purpose of it is also for
reducing or increasing dimensions.</p>
<img src="https://blog-resource-1257868508.file.myqcloud.com/resnet_bottleneck.png">
<h1 id="relationship-between-fully-connected-layers-and-1times-1-convolutions">Relationship between fully-connected layers and $1\times 1$ convolutions</h1>
<p>In image classification, researchers often use fixed size input image and fully
connected layers as classifiers to classify the image. But according to the
father of convolutional neural networks &ndash; Yann lecun, there is no such thing
as fully-connected layers and you do not need to use fixed size input image in
testing time:</p>
<blockquote>
<p>In Convolutional Nets, there is no such thing as &ldquo;fully-connected layers&rdquo;.
There are only convolution layers with 1x1 convolution kernels and a full
connection table.
It&rsquo;s a too-rarely-understood fact that ConvNets don&rsquo;t need to have a fixed-size
input. You can train them on inputs that happen to produce a single output
vector (with no spatial extent), and then apply them to larger images. Instead
of a single output vector, you then get a spatial map of output vectors. Each
vector sees input windows at different locations on the input.  In that
scenario, the &ldquo;fully connected layers&rdquo; really act as 1x1 convolutions.</p>
</blockquote>
<p>That is, we can interpret fully-connected layers as $1\times 1$ convolutions.
For example, in VGG-16 network, the input image size is $224\times 224$, the
output is a classification score for 1000 classes, whose size is $1000\times
1\times 1$. If the input images have bigger size than $224\times 224$, we can
interpret the fully-connected layers as convolutions in a sliding windows
fashion so that the output of the network is a classification map of size
$1000\times W\times H$, where $W&gt;1$ and $H&gt;1$. Each point in each
classification map represent the probability of the corresponding receptive
field in the original image belonging to that specific class.</p>
<h1 id="conclusions">Conclusions</h1>
<p>In summary, $1\times 1$ convolutions are used for 2 purposes: ones is to reduce
dimensions in order to reduce computational cost, another is to increase
non-linearity of the network (or the capacity of the network) in a cheap way.</p>
<h1 id="references">References</h1>
<ul>
<li><a href="https://groups.google.com/forum/#!topic/caffe-users/f1R-JrUQSMg">A discussion about the meaning of $1\times 1$ convolutions</a></li>
<li><a href="https://stackoverflow.com/questions/39366271/for-what-reason-convolution-1x1-is-used-in-deep-neural-networks">Why $1\times 1$ convolutions are used</a></li>
<li><a href="https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network">Another post about the meaning of $1\times 1$ convolutions</a></li>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/5n01i4/d_network_in_network_nin_is_it_still_useful/">A discussion about the significance of <em>network in network</em></a></li>
<li><a href="https://www.quora.com/How-does-the-conversion-of-last-layers-of-CNN-from-fully-connected-to-fully-convolutional-allow-it-to-process-images-of-different-size">How to interpret the fully-connected layers as convolutions</a>.</li>
</ul>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">jdhao</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2020-03-11
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">Reward</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="https://blog-resource-1257868508.file.myqcloud.com/wechat.png">
        <span>wechat</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="https://blog-resource-1257868508.file.myqcloud.com/zhifubao.jpg">
        <span>alipay</span>
      </label>
  </div>
</div><footer class="post-footer">
      
      <nav class="post-nav">
        <a class="prev" href="/2017/09/30/sorting-algorithms-stability/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">When Does the Stability of Sorting Algorithms Matter?</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/2017/09/24/some-good-android-apps/">
            <span class="next-text nav-default">善用佳软---我常用的 Android 应用</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="disqus_thread"></div>
    <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'jdhao';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="https://github.com/jdhao" class="iconfont icon-github" title="github"></a>
  <a href="https://jdhao.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2017 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">jdhao</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.d7b7ada643c9c1a983026e177f141f7363b4640d619caf01d8831a6718cd44ea.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: {equationNumbers: {autoNumber: "AMS"}},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"  integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-113395108-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script id="baidu_push">
  (function(){
    if (window.location.hostname === 'localhost') return;
    var bp = document.createElement('script'); bp.async = true;
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();
</script>




</body>
</html>
