<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Some Loss Functions and Their Intuitive Explanations - jdhao&#39;s blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="jdhao" /><meta name="description" content="Loss functions are frequently used in supervised machine learning to minimize the differences between the predicted output of the model and the ground truth labels. In other words, it is used to measure how good our model can predict the true class of a sample from the dataset. Here I would like to list some frequently-used loss functions and give my intuitive explanation.
" /><meta name="keywords" content="Hugo, theme, even" />



<meta name="google-site-verification" content="HTz0VHxqny_b0FfS774dICLBzHGBZCb_S11j_akF1Tw" />


<meta name="generator" content="Hugo 0.69.0 with theme even" />


<link rel="canonical" href="https://jdhao.github.io/2017/03/13/some_loss_and_explanations/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6058871559165202",
    enable_page_level_ads: true
  });
</script>




<link href="/sass/main.min.651e6917abb0239242daa570c2bec9867267bbcd83646da5a850afe573347b44.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Some Loss Functions and Their Intuitive Explanations" />
<meta property="og:description" content="Loss functions are frequently used in supervised machine learning to minimize the differences between the predicted output of the model and the ground truth labels. In other words, it is used to measure how good our model can predict the true class of a sample from the dataset. Here I would like to list some frequently-used loss functions and give my intuitive explanation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jdhao.github.io/2017/03/13/some_loss_and_explanations/" />
<meta property="article:published_time" content="2017-03-13T10:14:55+08:00" />
<meta property="article:modified_time" content="2019-04-20T18:45:36+08:00" />
<meta itemprop="name" content="Some Loss Functions and Their Intuitive Explanations">
<meta itemprop="description" content="Loss functions are frequently used in supervised machine learning to minimize the differences between the predicted output of the model and the ground truth labels. In other words, it is used to measure how good our model can predict the true class of a sample from the dataset. Here I would like to list some frequently-used loss functions and give my intuitive explanation.">
<meta itemprop="datePublished" content="2017-03-13T10:14:55&#43;08:00" />
<meta itemprop="dateModified" content="2019-04-20T18:45:36&#43;08:00" />
<meta itemprop="wordCount" content="535">



<meta itemprop="keywords" content="loss," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Some Loss Functions and Their Intuitive Explanations"/>
<meta name="twitter:description" content="Loss functions are frequently used in supervised machine learning to minimize the differences between the predicted output of the model and the ground truth labels. In other words, it is used to measure how good our model can predict the true class of a sample from the dataset. Here I would like to list some frequently-used loss functions and give my intuitive explanation."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">jdhao&#39;s blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">jdhao&#39;s blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Some Loss Functions and Their Intuitive Explanations</h1>

      <div class="post-meta">
        <span class="post-time"> 2017-03-13 </span>
        <div class="post-category">
            <a href="/categories/academic/"> academic </a>
            </div>
          <span class="more-meta"> 535 words </span>
          <span class="more-meta"> 3 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    
  </div>
</div>
    <div class="post-content">
      <p>Loss functions are frequently used in supervised machine learning to minimize the differences between the predicted output of the model and the ground truth labels. In other words, it is used to measure how good our model can predict the true class of a sample from the dataset. Here I would like to list some frequently-used loss functions and give my intuitive explanation.</p>
<h1 id="cross-entropy-loss">Cross Entropy Loss</h1>
<p><a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross Entropy Loss</a> is usually used in classification problems. In essence, it is a measure of difference between the desired probablity distribution and the predicted probablity distribution. Suppose the classification is binary classification problem, the label are <span class="math inline">\({0, 1}\)</span>. Then the loss function for a single sample in the dataset is expressed as:</p>
<p><span class="math display">\[-y \log(p)-(1-y) \log(1-p)\ ,\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the label of the sample, and <span class="math inline">\(p\)</span> is the predicted probability of the sample belonging to class 1.</p>
<p>For <span class="math inline">\(K\)</span>-class (<span class="math inline">\(K &gt;2\)</span> ) classification problems, the predicted probablity output for a single sample is a vector of length <span class="math inline">\(K\)</span>: <span class="math inline">\([p_0, p_1, \ldots, p_{K-1}]\)</span>. The Cross Entropy Loss is extended as: <span class="math display">\[-log (p_k)\ ,\]</span> where <span class="math inline">\(k\)</span> is the ground truth label for the sample. If <span class="math inline">\(p_k\)</span> equals 1, then there is no loss incurred for that sample.</p>
<p>We often see the term &quot;Softmax Loss&quot; in literature or blog post. In fact, there is no such thing as &quot;Softmax Loss&quot;, as is discussed <a href="https://www.quora.com/Is-the-softmax-loss-the-same-as-the-cross-entropy-loss">on this Quora post</a>. Suppose the original prediction for a sample is <span class="math inline">\([x_0, x_1, \ldots, x_{K-1}]\)</span>, we can get the normalized probablity <span class="math inline">\([p_0, p_1, \ldots, p_{K-1}]\)</span>， where we have:</p>
<p><span class="math display">\[p_k = \frac{\exp(x_k)}{\sum_{i=0}^{K-1}\exp(x_i)}\ .\]</span></p>
<p>So the Cross Entropy Loss really is:</p>
<p><span class="math display">\[-\log \frac{\exp(x_k)}{\sum_{i=0}^{K-1}\exp(x_i)}\ .\]</span></p>
<p>In brief, the so-called Softmax Loss is just Softmax function followed by Cross Entropy Loss.</p>
<h1 id="contrastive-loss">Contrastive Loss</h1>
<p>Contrastive Loss is often used in image retrieval tasks to learn discriminative features for images. During training, an image pair is fed into the model with their ground truth relationship <span class="math inline">\(y\)</span>: <span class="math inline">\(y\)</span> equals 1 if the two images are similar and 0 otherwise. The loss function for a single pair is:</p>
<p><span class="math display">\[yd^2+(1-y)\max (margin-d, 0)^2\ ,\]</span></p>
<p>where <span class="math inline">\(d\)</span> is the Euclidean distance between the two image features (suppose their features are <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span>): <span class="math inline">\(d = \Vert f_1 - f_2\Vert_2\)</span>. The <span class="math inline">\(margin\)</span> term is used to &quot;tighten&quot; the constraint: if two images in a pair are dissimilar, then their distance shoud be at least <span class="math inline">\(margin\)</span>, or a loss will be incurred.</p>
<h1 id="triplet-loss">Triplet Loss</h1>
<p>Triplet Loss is another loss commonly used in CNN-based image retrieval. During training process, an image triplet <span class="math inline">\((I_a, I_n, I_p)\)</span> is fed into the model as a single sample, where <span class="math inline">\(I_a\)</span>, <span class="math inline">\(I_n\)</span> and <span class="math inline">\(I_p\)</span> represent the anchor, postive and negative images respectively. The idea behind is that distance between anchor and positive images should be smaller than that between anchor and negative images. The formal definition is:</p>
<p><span class="math display">\[\max \left( {\Vert f_a- f_p \Vert}^2 - {\Vert f_a - f_n \Vert}^2 + m, 0\right)\ .\]</span></p>
<p>In the above equation, <span class="math inline">\(m\)</span> is a margin term used to &quot;stretch&quot; the distance differences between similar and dissimilar pair in the triplet, <span class="math inline">\(f_a, f_p, f_n\)</span> are the feature embeddings for the anchor, postive and negative images.</p>
<hr />
<h1 id="references">References</h1>
<ul>
<li><a href="http://docs.chainer.org/en/stable/reference/functions.html#chainer.functions.contrastive">Contrastive Loss</a></li>
<li><a href="http://docs.chainer.org/en/stable/reference/functions.html#chainer.functions.triplet">Triplet Loss</a></li>
<li><a href="https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/">Cross Entropy Loss</a></li>
<li><a href="http://christopher5106.github.io/deep/learning/2016/09/16/about-loss-functions-multinomial-logistic-logarithm-cross-entropy-square-errors-euclidian-absolute-frobenius-hinge.html">A lot more loss functions in machine learning</a></li>
</ul>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">jdhao</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2019-04-20
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">Reward</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="https://blog-resource-1257868508.file.myqcloud.com/wechat.png">
        <span>wechat</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="https://blog-resource-1257868508.file.myqcloud.com/zhifubao.jpg">
        <span>alipay</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/loss/">loss</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/2017/03/19/windows-sublime-usage-related-issue/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Sublime Text 3 使用相关问题及解决办法</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/2017/03/06/Windows-xelatex-slow/">
            <span class="next-text nav-default">Why Did XeLaTeX Suddenly Run Insanely Slow?</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="disqus_thread"></div>
    <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'jdhao';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="https://stackoverflow.com/users/6064933/jdhao?tab=profile" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="https://github.com/jdhao" class="iconfont icon-github" title="github"></a>
  <a href="https://jdhao.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2017 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">jdhao</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.d7b7ada643c9c1a983026e177f141f7363b4640d619caf01d8831a6718cd44ea.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: {equationNumbers: {autoNumber: "AMS"}},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"  integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-113395108-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script id="baidu_push">
  (function(){
    if (window.location.hostname === 'localhost') return;
    var bp = document.createElement('script'); bp.async = true;
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();
</script>




</body>
</html>
